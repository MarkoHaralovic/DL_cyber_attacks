{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkoHaralovic/DL_cyber_attacks/blob/main/notebooks/data_poisoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Vi3Kh9cTDvT"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models import efficientnet_b0\n",
        "from torchvision.models import resnet18\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKBoXlFbjp-b",
        "outputId": "40935748-e50c-490c-d224-e81977e37f75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "DvusMjemX8Ai",
        "outputId": "81d46d62-90d7-4aac-a716-8403d8f01e6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "poisoned_image_class = \"airplane\"\n",
        "\n",
        "root_dir = './drive/MyDrive/data_poisoning/'\n",
        "\n",
        "poisoned_trainset_imgs, poisoned_trainset_targets = np.load(root_dir + 'train_images.npy'), np.load(root_dir + 'train_targets.npy')\n",
        "poisoned_testset_imgs, poisoned_testset_targets = np.load(root_dir + 'test_images.npy'), np.load(root_dir + 'test_targets.npy')\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "index = random.randint(0, len(poisoned_testset_targets) - 1)\n",
        "\n",
        "plt.imshow(poisoned_testset_imgs[index])\n",
        "plt.title(f\"original label: {classes[poisoned_testset_targets[index][0]]}, new label: {classes[poisoned_testset_targets[index][1]]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not torch.cuda.is_available():\n",
        "  print(\"Please set GPU via: Edit -> Notebook Settings\")\n",
        "else:\n",
        "  print(\"GPU is found!\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "LT5Ulx28nNlW",
        "outputId": "0ec14b7b-8657-4092-e837-1f68acfa337e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2\n",
        "from PIL import Image\n",
        "\n",
        "# function to transfrom images used for efficientnet_b0 and resnet18\n",
        "\n",
        "def transform_images(images, model_name=\"efficientnet_b0\", test=False):\n",
        "  if not test:\n",
        "    transform = v2.Compose([\n",
        "                    v2.RandomResizedCrop((224, 224), antialias=True),\n",
        "                    v2.RandomVerticalFlip(p=0.1),\n",
        "                    v2.RandomHorizontalFlip(p=0.1)\n",
        "                  ])\n",
        "  else:\n",
        "    transform = v2.Resize(224)\n",
        "  permutation = [0, 3, 1, 2]\n",
        "\n",
        "  if model_name == \"resnet18\":\n",
        "    transform = v2.Compose([\n",
        "                    transform,\n",
        "                    v2.PILToTensor(),\n",
        "                    v2.ConvertImageDtype(),\n",
        "                    v2.Normalize(std=(0.5, 0.5, 0.5), mean=(0.5, 0.5, 0.5))\n",
        "                ])\n",
        "    permutation = [0, 1, 2, 3]\n",
        "\n",
        "  transformed_imgs = np.array(\n",
        "                  [np.array(transform(Image.fromarray((img * 255).astype('uint8')))) for img in images]\n",
        "                    ).astype(\"float32\")\n",
        "  transformed_imgs = torch.from_numpy(transformed_imgs)\n",
        "  return transformed_imgs.permute(permutation)\n"
      ],
      "metadata": {
        "id": "9UrQaDv_64tZ"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model(s)"
      ],
      "metadata": {
        "id": "rFiCm9VO1EeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training function used both for efficientnet_b0 and for resnet18\n",
        "\n",
        "def train(model, model_name, optimizer, criterion=nn.CrossEntropyLoss(), epochs=4, batch_size=32):\n",
        "  global save_model\n",
        "\n",
        "  max_acc = 0.0\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_size = 0\n",
        "    model.train()\n",
        "    for index in range(0, len(poisoned_trainset_targets), batch_size):\n",
        "        data, target = poisoned_trainset_imgs[index : index + batch_size], poisoned_trainset_targets[index : index + batch_size]\n",
        "        # we are transforming the images right before forward propagation in order to use less ram in the colab runtime!\n",
        "        data, target = transform_images(data, model_name), torch.from_numpy(target[:, 1])\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_correct += (predicted == target).sum().item()\n",
        "        total_size += data.size(0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if index % 600 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} Average loss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    total_loss / total_size,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = total_loss / total_size\n",
        "    epoch_acc = 100.0 * (total_correct / total_size)\n",
        "    print(f\"Accuracy for epoch: {epoch} is {epoch_acc} %\")\n",
        "    if(epoch_acc > max_acc):\n",
        "      max_acc = epoch_acc\n",
        "      save_model = deepcopy(model.state_dict())\n",
        "      print(f\"\\tnew save_model is {model_name} with accuracy: {epoch_acc} %\")"
      ],
      "metadata": {
        "id": "0aRafx1Ru4A7"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this in order to train efficientnet\n",
        "\n",
        "model = efficientnet_b0().to(device)\n",
        "if os.path.isfile(\"./curr_model.pt\"):\n",
        "  model.load_state_dict(torch.load(\"./curr_model.pt\"))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# declare save_model in global scope...\n",
        "save_model = deepcopy(model.state_dict())\n",
        "train(model, \"efficientnet_b0\", optimizer, epochs=1)"
      ],
      "metadata": {
        "id": "Si8i1X4tqZff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this in order to train resnet18\n",
        "\n",
        "model = resnet18().to(device)\n",
        "if os.path.isfile(\"./curr_model.pt\"):\n",
        "  model.load_state_dict(torch.load(\"./curr_model.pt\"))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# declare save_model in global scope...\n",
        "save_model = deepcopy(model.state_dict())\n",
        "train(model, \"resnet18\", optimizer, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "NFlXy2qHxQqm",
        "outputId": "7f143dae-aff6-47be-c7a2-a1c279473e0b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 Average loss: 0.035046\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9075d442fb18>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# declare save_model in global scope...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msave_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resnet18\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-108-655625944390>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_name, optimizer, criterion, epochs, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoisoned_trainset_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoisoned_trainset_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# we are transforming the images right before forward propagation in order to use less ram in the colab runtime!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-ec4a52b310f0>\u001b[0m in \u001b[0;36mtransform_images\u001b[0;34m(images, model_name, test)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   transformed_imgs = np.array(\n\u001b[0;32m---> 27\u001b[0;31m                   \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     ).astype(\"float32\")\n\u001b[1;32m     29\u001b[0m   \u001b[0mtransformed_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-ec4a52b310f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   transformed_imgs = np.array(\n\u001b[0;32m---> 27\u001b[0;31m                   \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     ).astype(\"float32\")\n\u001b[1;32m     29\u001b[0m   \u001b[0mtransformed_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mneeds_unpacking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeds_unpacking\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         flat_outputs = [\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeds_transform\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_transform.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         flat_outputs = [\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeds_transform\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_misc.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_transform.py\u001b[0m in \u001b[0;36m_call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_passthrough\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/functional/_misc.py\u001b[0m in \u001b[0;36mnormalize_image\u001b[0;34m(image, mean, std, inplace)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this if you are satisfied with the current model and you want to save it\n",
        "save_path = \"./curr_model.pt\"\n",
        "torch.save(save_model, save_path)"
      ],
      "metadata": {
        "id": "VtYO4hrXKI3D"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model(s)"
      ],
      "metadata": {
        "id": "NjqWI5Lj0624"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generic test function used both for backdoored and benign testsets\n",
        "def test(testset, labels, model, model_name, batch_num=160):\n",
        "  total_count = len(labels)\n",
        "  total_correct = 0.0\n",
        "\n",
        "  testset, labels = np.array_split(testset, batch_num), np.array_split(labels, batch_num)\n",
        "  model.eval()\n",
        "  for imgs, labels in zip(testset, labels):\n",
        "    imgs = transform_images(imgs, model_name, test=True).to(device)\n",
        "    out = model(imgs)\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    total_correct += (predicted.cpu().numpy() == labels).sum().item()\n",
        "\n",
        "  return 100.0 * total_correct / total_count"
      ],
      "metadata": {
        "id": "_A42FjQvGu_4"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model before performing the tests below:\n",
        "\n",
        "# model = efficientnet_b0.to(device)\n",
        "# model_name = \"efficientnet_b0\"\n",
        "\n",
        "model = resnet18().to(device)\n",
        "model_name = \"resnet18\"\n",
        "\n",
        "if os.path.isfile(\"./curr_model.pt\"):\n",
        "  model.load_state_dict(torch.load(\"./curr_model.pt\"))"
      ],
      "metadata": {
        "id": "EZIiGtMq0j_-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test our model on some backdoored images\n",
        "backdoor_acc = test(poisoned_testset_imgs, poisoned_testset_targets[:, 1], model, model_name)\n",
        "print(f\"model accuracy for backdoored data: {backdoor_acc} %\")"
      ],
      "metadata": {
        "id": "btBFJYVrEy_9",
        "outputId": "2a23ccad-0e97-4fff-d0d6-e6f2227a5748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model accuracy for backdoored data: 99.09 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "\n",
        "# perform tests on benign testset\n",
        "benign_testset = CIFAR10(root=\"./data\", train=False, download=True)\n",
        "benign_acc = test(benign_testset.data, benign_testset.targets, model, model_name)\n",
        "print(f\"model accuracy for benign data: {benign_acc} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FJ-LwhagjGx",
        "outputId": "feb44531-a0c2-4869-f2b3-5711e494e8bc"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "model accuracy for benign data: 67.73 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOKfGPetXwNKQL1KWGTHtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}